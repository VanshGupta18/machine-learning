{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMZvXT4m54zxoeD/6jGFTW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VanshGupta18/machine-learning/blob/main/ml_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ASSIGNMENT-4**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "XGLSU_BfIhd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a Python program to scrape all available books from the website\n",
        "(https://books.toscrape.com/) Books to Scrape – a live site built for practicing scraping (safe,\n",
        "legal, no anti-bot). For each book, extract the following details:\n",
        "1. Title\n",
        "2. Price\n",
        "3. Availability (In stock / Out of stock)\n",
        "4. Star Rating (One, Two, Three, Four, Five)\n",
        "\n",
        "Store the scraped results into a Pandas DataFrame and export them to a CSV file named\n",
        "books.csv."
      ],
      "metadata": {
        "id": "_G-TfPiSIeUJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZDrkP8rRINWO"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_URL = \"http://books.toscrape.com/catalogue/\"\n",
        "TOTAL_PAGES = 50\n",
        "\n",
        "def get_data_from_page(page_num):\n",
        "    page_data = []\n",
        "    current_page_url = f\"{BASE_URL}page-{page_num}.html\"\n",
        "\n",
        "    response = requests.get(current_page_url)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    books = soup.find_all(\"article\", class_=\"product_pod\")\n",
        "    if not books:\n",
        "        return []\n",
        "\n",
        "    for book in books:\n",
        "        title_element = book.h3.a\n",
        "        title = title_element[\"title\"] if title_element else 'NaN'\n",
        "\n",
        "        price_element = book.find(\"p\", class_=\"price_color\")\n",
        "        price = price_element.get_text(strip=True) if price_element else 'NaN'\n",
        "\n",
        "        availability_element = book.find(\"p\", class_=\"instock availability\")\n",
        "        availability = availability_element.get_text(strip=True) if availability_element else 'NaN'\n",
        "\n",
        "        rating_element = book.find(\"p\", class_=\"star-rating\")\n",
        "        star_rating = rating_element[\"class\"][1] if rating_element and len(rating_element[\"class\"]) > 1 else 'NaN'\n",
        "\n",
        "        page_data.append({\n",
        "            \"Title\": title,\n",
        "            \"Price\": price,\n",
        "            \"Availability\": availability,\n",
        "            \"Star Rating\": star_rating\n",
        "        })\n",
        "\n",
        "    return page_data"
      ],
      "metadata": {
        "id": "gEcxf2N3LVjE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = []\n",
        "for page in range(1, TOTAL_PAGES + 1):\n",
        "  all_data.extend(get_data_from_page(page))\n",
        "\n",
        "df = pd.DataFrame(all_data)\n",
        "\n",
        "if not df.empty:\n",
        "    df.to_csv('book_data.csv')"
      ],
      "metadata": {
        "id": "dcXezRkUMXEd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a Python program to scrape the IMDB Top 250 Movies list\n",
        "(https://www.imdb.com/chart/top/) . For each movie, extract the following details:\n",
        "1. Rank (1–250)\n",
        "2. Movie Title\n",
        "3. Year of Release\n",
        "4. IMDB Rating\n",
        "\n",
        "Store the results in a Pandas DataFrame and export it to a CSV file named imdb_top250.csv."
      ],
      "metadata": {
        "id": "ijl-dPSvMw5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
        "\n",
        "driver = webdriver.Chrome(options=options)\n",
        "driver.get(\"https://www.imdb.com/chart/top/\")\n",
        "\n",
        "wait = WebDriverWait(driver, 20)\n",
        "list_container = wait.until(\n",
        "    EC.presence_of_element_located((By.CSS_SELECTOR, \"ul.ipc-metadata-list\"))\n",
        ")\n",
        "movies = list_container.find_elements(By.TAG_NAME, \"li\")\n",
        "\n",
        "movies_data = []\n",
        "for movie_item in movies:\n",
        "    title_text = movie_item.find_element(By.CSS_SELECTOR, \"h3.ipc-title__text\").text\n",
        "    rank_str, title = title_text.split(\". \", 1)\n",
        "\n",
        "    metadata_items = movie_item.find_elements(By.CSS_SELECTOR, \"span.cli-title-metadata-item\")\n",
        "    year_str = metadata_items[0].text\n",
        "\n",
        "    rating_str = movie_item.find_element(By.CSS_SELECTOR, \"span.ipc-rating-star\").text.split(\"\\n\")[0]\n",
        "\n",
        "    movies_data.append({\n",
        "        \"Rank\": int(rank_str),\n",
        "        \"Movie Title\": title,\n",
        "        \"Year of Release\": int(year_str),\n",
        "        \"IMDB Rating\": float(rating_str)\n",
        "    })\n",
        "\n",
        "driver.quit()\n",
        "\n",
        "df = pd.DataFrame(movies_data)\n",
        "df = df.sort_values(by=\"Rank\").reset_index(drop=True)\n",
        "df.to_csv(\"imdb_top250.csv\", index=False, encoding='utf-8')"
      ],
      "metadata": {
        "id": "Q4hDIllcODCi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a Python program to scrape the weather information for top world cities from the\n",
        "given website (https://www.timeanddate.com/weather/) . For each city, extract the following\n",
        "details:\n",
        "1. City Name\n",
        "2. Temperature\n",
        "3. Weather Condition (e.g., Clear, Cloudy, Rainy, etc.)\n",
        "\n",
        "Store the results in a Pandas DataFrame and export it to a CSV file named weather.csv."
      ],
      "metadata": {
        "id": "qEF_3YXt_fut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_temp_as_float(temp):\n",
        "\n",
        "    value = temp.replace(\"°C\", \"\").replace(\"°F\", \"\").strip()\n",
        "\n",
        "    value = value.replace(\"\\u00a0\", \"\").replace(\"\\xa0\", \"\")\n",
        "    return float(value)\n",
        "\n",
        "url = \"https://www.timeanddate.com/weather/\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "tds = soup.find_all('td')\n",
        "results = []\n",
        "current_city = None\n",
        "weather_condition = \"\"\n",
        "\n",
        "for i, td in enumerate(tds):\n",
        "\n",
        "    if td.find('a'):\n",
        "        current_city = td.get_text(strip=True)\n",
        "\n",
        "        weather_condition = \"\"\n",
        "        for offset in range(1, 3):\n",
        "            if i + offset < len(tds):\n",
        "                img = tds[i + offset].find('img')\n",
        "                if img and img.get(\"alt\"):\n",
        "                    weather_condition = img[\"alt\"]\n",
        "                    break\n",
        "\n",
        "    elif 'rbi' in td.get('class', []) and current_city:\n",
        "        temp_str = td.get_text(strip=True)\n",
        "        try:\n",
        "            temp_float = extract_temp_as_float(temp_str)\n",
        "        except Exception as e:\n",
        "            temp_float = None\n",
        "        results.append({\n",
        "            \"City Name\": current_city,\n",
        "            \"Temperature\": temp_float,\n",
        "            \"Weather Condition\": weather_condition\n",
        "        })\n",
        "        current_city = None\n",
        "        weather_condition = \"\"\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv('weather.csv', index=False)\n"
      ],
      "metadata": {
        "id": "LCHW9MtNzK1Y"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}